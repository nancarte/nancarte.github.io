---
layout: post
title:  "LIME: Local Interpretable Model Agnostic Explanations"
date:   2023-05-03
author: Nathaniel Carter
description: Exploring ATP Tour Data
image: /assets/images/download.jpg
---

### Introduction

Machine Learning is used to solve important problems every day but for the most part machine learning models are black box models. This means we get to see the data and the predictions/classifications but we usually don't get to see how exactly the predictions/classifications were made. It can be helpful to see the importance of features to explain a model in order to trust the model. Without seeing the importance of certain features of the model it can be hard to know whether to trust or not trust a model. Because it is useful to see how a machine learning model is using the data to make predictions a model called LIME was created and explained in the paper "Why Should I Trust You?". LIME isn't the only model that helps explain black box models and another one that is well known is SHAP. The information about LIME from the creators of the model is found in this paper so I am using it for much of this project.

### What is LIME?
LIME is an acronym for Local Interpretable Model-Agnostic Explanations.

Local means that the explanation should show what is actually happening around the local data point that is being classified.

Interpretable means that the explanations are going to be able to be interpreted by a human.

Model-Agnostic means that it can be used with any classification model and it doesn't look at the model when making explanations.

### Example of Usefulness of LIME
In the paper explaining LIME an example is shown where a classifier achieves high accuracy but actually is a bad classifier. It is a bad classifier because the words that are used for the classifier have nothing to do with what is being classified. The paper shows an example of a classifier that is trying to classify whether what is written is about Christianity or Atheism. In the paper an example is shown where the word "posting" 99% of the time is in the class Atheism which is one word that leads to a bad classifier. This classifier wouldn't be useful for making general predictions about Christianity or Atheism. Without LIME someone could just trust the model is good because of the high accuracy but later on would realize that the model isn't working well.


### Lab

Below I will be going through an example to help see how LIME is used.

### 1. Load Penguins Data and Packages

```
import pandas as pd
  from sklearn.model_selection import train_test_split
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.metrics import accuracy_score
  df = pd.read_csv('penguins.csv')

```
  

