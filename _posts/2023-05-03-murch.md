---
layout: post
title:  "Murch Survey Data Analysis"
date:   2023-05-03
author: Nathaniel Carter
description: Murch Data Analysis
image: /assets/images/download.jpg
---

### Introduction

While working at Murch as an intern I and four other interns created a survey that asked what people thought of Murch. After obtaining the data I analyzed the data and did text analysis as well as Logistic Regression. The text analysis I did included creating unigram bar charts and using BERTopic for Topic Modeling.

### Unigram Bar Charts

The bar charts I created are the top ten unigrams of responses from specific questions. The first one is a unigram for clothing types.

### Clothing Type Unigram

This unigram identifies shirts, dresses, and shoes as the top three clothing items that those that were taking the survey want to be on Murch. Dresses and shoes were not on Murch before we took this survey so it is helpful information for Murch to consider adding these clothing types to their website.

![Figure](https://raw.githubusercontent.com/nancarte/nancarte.github.io/master/posts/images/Picture2.png)

### Inconveniences Unigram

This unigram shows that trying clothes on and dealing with the fit/size are the main inconveniences people have about shopping online.

![Figure](https://raw.githubusercontent.com/nancarte/nancarte.github.io/master/posts/images/Picture3.png)

### Social Media Unigram

This unigram shows that by far Instagram is the most popular form of social media for buying clothes online. Because of this Murch should initially focus on Instagram for social media and as they grow expand where they post on social media.

![Figure](https://raw.githubusercontent.com/nancarte/nancarte.github.io/master/posts/images/Picture4.png)

### Topic Modeling

Further text analysis was done with BERTopic which attempts to create topics out of the text. The first visualization created topics from the responses about what the company did. Topic 0 could be labeled as Murch because the words are things that Murch does. Topic 1 is showing that there were responses like yes, think, yeah, kind, sort which shows that some of the responses weren’t really descriptions  but more bad responses to the question.

![Figure](https://raw.githubusercontent.com/nancarte/nancarte.github.io/master/posts/images/Picture5.png)

The second visualization created topics from the responses about inconveniences shopping online. Topic 0 could be labeled clothing fit because it is showing that people have inconveniences by using those kinds of words. Topic 1 could be inability to try on clothes because the response try is showing that those that responded to the survey want to try on clothes but are unable to when buying online. Topic 2 could be problems with shipping/delivery.

![Figure](https://raw.githubusercontent.com/nancarte/nancarte.github.io/master/posts/images/Picture6.png)

### Logistic Regression

To see if we were able to classify whether someone would put down an email or not based on the responses a classification model was created. For this problem a Logistic Regression model was used. Many of the questions from the survey didn’t seem suitable for this problem. So only the questions about demographics and how often someone shopped were used. Dummy variables were created which makes categorical variables into 1s and 0s. For the response variable 1 meant they put down their email. The data was first split into training and test data. Then the model was fit to the training data. Predictions were then made using the test data. Below is an image of the the real values and what the predictions were.

![Figure](https://raw.githubusercontent.com/nancarte/nancarte.github.io/master/posts/images/Picture7.png)

The classification report shows that the accuracy was around 66% which is okay. But the f1-score which is used to see how the classification model worked overall was .77 for 0s and .32 for 1s. This means the model is much better at classifying no email versus an email. Part of the reason this model didn’t work out as well as we would like is that there is imbalance in the data. There are many more responses that have no emails versus an email. Also perhaps some of the variables that were taken out could be used to help the model. Overall there probably needs to be more data and better variables to predict whether someone will put down an email or not. Below is a confusion matrix

![Figure](https://raw.githubusercontent.com/nancarte/nancarte.github.io/master/posts/images/Picture8.png)

This is helpful for seeing that the model is bad at predicting 1s or those that put down their email. For Murch to create a model to predict whether someone will put down their email or not they would need to gather more data with better questions to help classify those that put down their email.

### Conclusion

There are other methods for Topic Modeling like LDA. Sometimes LDA works better and sometimes BERTopic is better so it is a good idea to try multiple methods to find the best one available. Text analysis can be helpful when applied correctly and while it would have been nice to have had more responses there still was some useful insights gained from this text analysis. Logistic Regression was used to see if a model could be fit to predict whether someone would put down their email or not. The data wasn't great for this purpose so if Murch wants to solve this problem they would likely need to do a survey with more responses and more relevant questions.


