---
layout: post
title:  "LIME: Local Interpretable Model Agnostic Explanations"
date:   2023-05-03
author: Nathaniel Carter
description: Exploring ATP Tour Data
image: /assets/images/download.jpg
---

### Introduction

Machine Learning is used to solve important problems every day but for the most part machine learning models are black box models. This means we get to see the data and the predictions/classifications but we usually don't get to see how exactly the predictions/classifications were made. It can be helpful to see the importance of features to explain a model in order to trust the model. Without seeing the importance of certain features of the model it can be hard to know whether to trust or not trust a model. Because it is useful to see how a machine learning model is using the data to make predictions a model called LIME was created and explained in the paper "Why Should I Trust You?". LIME isn't the only model that helps explain black box models and another one that is well known is SHAP. The information about LIME from the creators of the model is found in this paper so I am using it for much of this project.

### What is LIME?
LIME is an acronym for Local Interpretable Model-Agnostic Explanations.

Local means that the explanation should show what is actually happening around the local data point that is being classified.

Interpretable means that the explanations are going to be able to be interpreted by a human.

Model-Agnostic means that it can be used with any classification model and it doesn't look at the model when making explanations.

### Example of Usefulness of LIME
In the paper explaining LIME an example is shown where a classifier achieves high accuracy but actually is a bad classifier. It is a bad classifier because the words that are used for the classifier have nothing to do with what is being classified. The paper shows an example of a classifier that is trying to classify whether what is written is about Christianity or Atheism. In the paper an example is shown where the word "posting" 99% of the time is in the class Atheism which is one word that leads to a bad classifier. This classifier wouldn't be useful for making general predictions about Christianity or Atheism. Without LIME someone could just trust the model is good because of the high accuracy but later on would realize that the model isn't working well.


### Lab

Below I will be going through an example to help see how LIME is used.

### 1. Load Penguins Data and Packages

```
  import pandas as pd
  from sklearn.model_selection import train_test_split
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.metrics import accuracy_score
  from sklearn.impute import SimpleImputer
  df = pd.read_csv('penguins.csv')
```

### 2. Data Cleaning

To clean the data I had to get rid of missing values. I imputed the most frequent values for the categorical data and imputed the mean for numerical data. I also created dummy variables.

```
df['sex'] = df['sex'].replace({'.': np.nan})
```
```
imputer = SimpleImputer(strategy='most_frequent')
imputer = imputer.fit(df[['sex']])
df[['sex']] = imputer.transform(df[['sex']])
```
```
df = pd.get_dummies(df, columns = ['island', 'sex'])
```
### 3. Fitting a model

Now with the x and y values I fit the data to a Random Forest Classifier

```
X1 = df.loc[:, 'culmen_length_mm':'sex_MALE']
y1 = df['species']
X = X1.values
y = y1.values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)
model = RandomForestClassifier(max_depth=2, random_state=1, n_estimators=5)
model.fit(X_train, y_train)
```
### 4. Lime
Finally here is code that has to do with LIME. To access LIME use pip install lime.
These are the required lime packages for the LIME visualizations

```
import lime
from lime import lime_tabular
```
This code is used to show a visualization of the feature importance from LIME.
```
explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=X1.columns, class_names=['Adelie','Chinstrap','Gintoo'])
exp = explainer.explain_instance(X1.iloc[5],model.predict_proba,top_labels=1)
exp.show_in_notebook(show_table=True, show_all=False)
```
This visualization below is showing that the model is 96% confident that this instance is an Adelie penguin. It is also showing what feautures led to the prediction of an Adelie. In this example the culmen_length_mm was the most important feature.

![Figure](https://raw.githubusercontent.com/nancarte/nancarte.github.io/master/posts/images/LIME.png)


Overall, LIME is a useful tool for understanding feature importance of a specific instance found in the data. SHAP is another tool that I didn't research but it seems to be just as useful as LIME for figuring out feature importance in black box models. The code for LIME is not very complicated as well which makes it very usable.



